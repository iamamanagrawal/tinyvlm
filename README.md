## TinyVLM

The aim of this repository to build minimal, hackable codebase to train a small vision language model with pretrained language model and pretrained vision encoder. This project is inspired from the works like HuggingFace's [nanoVLM](https://github.com/huggingface/nanoVLM) repository and [LLaVa](https://arxiv.org/pdf/2304.08485).

It took nearly 1$ to train the modality projection! I had rented a A40 GPU from [runpod](https://www.runpod.io) for the work.

### Why?

Small models are very useful for the following reasons -
- Inference costs are low
- Data Privacy and security when running on local devices
- Lower latency
- Perform at par with larger models in specific tasks

My curiousity was whether the small language models which were pretrained just on text (text-in, text-out), can they visually see and percieve the world as we humans do.

The total number of parameters of the proposed model is roughly 300 Million!

### Methodology

The methodology uses [SmolLM2-Instruct](https://huggingface.co/HuggingFaceTB/SmolLM2-135M-Instruct) model by HuggingFace and [SigLip](https://huggingface.co/google/siglip-base-patch16-224) by Google. The idea was to learn a mapping in between image embeddings generated by the visual encoder with the text embedding space by a projector which we call it as <b><u>Modality Projector</b></u>. This codebase uses <b>SwiGLU</b> MLP layer which has become a standard replacement of naive MLP in modern transformer model training.

The objective of the model training was - given an image and user prompt, learn the weights of the modality projector such that the text generation as an assistant's text matches the ground truth caption.

[LLaVA-CC3M-Pretrain-595K](https://huggingface.co/datasets/liuhaotian/LLaVA-CC3M-Pretrain-595K) was used for training the projector. This is the same dataset which was used in pretraining of the LLaVa Model.

### Run

Download the dataset and the weights of the langauge model and visual encoder.

```bash
git lfs install
git clone https://huggingface.co/datasets/liuhaotian/LLaVA-CC3M-Pretrain-595K
git clone https://huggingface.co/HuggingFaceTB/SmolLM2-135M-Instruct
git clone https://huggingface.co/google/siglip-base-patch16-224

mkdir data models
mv LLaVA-CC3M-Pretrain-595K/ data/
mv SmolLM2-135M-Instruct/ models/
mv siglip-base-patch16-224/ models/
```

Using uv, create a virtual environment

```bash
uv sync
```

Train the model projection by running the following command

```bash
python main.py
```

Note: Make sure you use w&b to monitor the training. If you want to change the configurations (batch size, gradient accumulation), you can take a look at ```VLMConfig``` in the ```main.py``` file.

### Results


| Training Loss | Validation Loss | Learning Rate | Gradient Noem |
|---|---|---|---|
| <img src="assets/train_loss.png" alt="Sample 1" width="400"> | <img src="assets/val_loss.png" alt="Sample 1" width="400"> | <img src="assets/lr.png" alt="Sample 1" width="400"> | <img src="assets/grad_norm.png" alt="Sample 1" width="400"> |
-----------


### ToDos

- [ ] Add a model inference script
- [ ] Finetune the model similar to Llava paper on downstream tasks
- [ ] Benchmark the results on downstream tasks
