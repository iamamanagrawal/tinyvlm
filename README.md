## TinyVLM

The aim of this repository to build minimal, hackable codebase to train a small vision language model with pretrained language model and pretrained vision encoder. This project is inspired from the works like HuggingFace's [nanoVLM](https://github.com/huggingface/nanoVLM) repository and [LLaVa](https://arxiv.org/pdf/2304.08485).

It took nearly 1$ to train the modality projection! I had rented a A40 GPU from [runpod](https://www.runpod.io) for the work.

### Why?

Small models are very useful for the following reasons -
- Inference costs are low
- Data Privacy and security when running on local devices
- Lower latency
- Perform at par with larger models in specific tasks

My curiousity was whether the small language models which were pretrained just on text (text-in, text-out), can they visually see and percieve the world as we humans do.

The total number of parameters of the proposed model is roughly 300 Million!

### Methodology

The methodology uses [SmolLM2-Instruct](https://huggingface.co/HuggingFaceTB/SmolLM2-135M-Instruct) model by HuggingFace and [SigLip](https://huggingface.co/google/siglip-base-patch16-224) by Google. The idea was to learn a mapping in between image embeddings generated by the visual encoder with the text embedding space by a projector which we call it as <b><u>Modality Projector</b></u>. This codebase uses <b>SwiGLU</b> MLP layer which has become a standard replacement of naive MLP in modern transformer model training.

The objective of the model training was - given an image and user prompt, learn the weights of the modality projector such that the text generation as an assistant's text matches the ground truth caption.

[LLaVA-CC3M-Pretrain-595K](https://huggingface.co/datasets/liuhaotian/LLaVA-CC3M-Pretrain-595K) was used for training the projector. This is the same dataset which was used in pretraining of the LLaVa Model.

### Run

Download the dataset and the weights of the langauge model and visual encoder.

```bash
git lfs install
git clone https://huggingface.co/datasets/liuhaotian/LLaVA-CC3M-Pretrain-595K
git clone https://huggingface.co/HuggingFaceTB/SmolLM2-135M-Instruct
git clone https://huggingface.co/google/siglip-base-patch16-224

mkdir data models
mv LLaVA-CC3M-Pretrain-595K/ data/
mv SmolLM2-135M-Instruct/ models/
mv siglip-base-patch16-224/ models/
```

Using uv, create a virtual environment

```bash
uv sync
```

Train the model projection by running the following command

```bash
python main.py
```

Note: Make sure you use w&b to monitor the training. If you want to change the configurations (batch size, gradient accumulation), you can take a look at ```VLMConfig``` in the ```main.py``` file.

The checkpoint for the modality projector can be found [here](https://drive.google.com/drive/folders/1UV8tv9igWMFoj1abi2cpNzfOhcB4F5Kx?usp=sharing).

### Results


| Training Loss | Validation Loss | Learning Rate | Gradient Norm |
|---|---|---|---|
| <img src="assets/train_loss.png" alt="Sample 1" width="400"> | <img src="assets/val_loss.png" alt="Sample 1" width="400"> | <img src="assets/lr.png" alt="Sample 1" width="400"> | <img src="assets/grad_norm.png" alt="Sample 1" width="400"> |
-----------

You can also test the inference and the performace by running ``` python -m tests.infer```. The following is the output generation.

```txt
=== Inference Example buildings.png ===
Image Path: buildings.png
Generated Text: the skyline of the city is a breathtaking sight, with the city skyline rising above the city skyline, creating a stunning view of the city skyline. the city skyline is a stunning sight, with the city skyline rising above the
=========================
=== Inference Example bird.png ===
Image Path: bird.png
Generated Text: The birds are perched on the branches of a cherry tree, their beaks and beaks of feathers glistening in the sunlight. The birds are perched on the branches of a cherry tree, their beaks and beaks of feathers glistening
=========================
=== Inference Example beach.png ===
Image Path: beach.png
Generated Text: The beach is a popular destination for beach lovers, and the beach itself is a popular destination for people who love to relax and unwind. The beach is a popular destination for people who love to relax and unwind. The beach is a popular destination
=========================
```

The result shows that the mapping is doing decent, atleast it tries to understand a high level understanding of images which is good considering the size of the model. But for some images, the model doesn't generate anything. So, there is a good room for improvement in terms of pretrained model choices, modality projector architecture, dataset chosen (since, its bit noisy) and training configurations!

#### Update 

I also tried using SmolLM2-360M-Instruct as the pretrained LLM to train the modality projector. Here are the training statistics.

| Training Loss | Validation Loss | Learning Rate | Gradient Norm |
|---|---|---|---|
| <img src="assets/train_loss_360m.png" alt="Sample 1" width="400"> | <img src="assets/val_loss_360m.png" alt="Sample 1" width="400"> | <img src="assets/train_lr_360m.png" alt="Sample 1" width="400"> | <img src="assets/grad_norm_360m.png" alt="Sample 1" width="400"> |
-----------

The model attained a higher loss in comparison to SmolLM2-135M though.

Here is how SmolLM2-360M-Instruct works on the examples images

```txt
=== Inference Example buildings.png ===
Image Path: buildings.png
Generated Text: the image is a time lapse of the sunset over the cityscape of the skyline of the city. The sky is painted with hues of orange and pink, and the clouds are streaked across the sky. The cityscape is illuminated by the sun
=========================
=== Inference Example bird.png ===
Image Path: bird.png
Generated Text: birds in a tree with a beautiful pink flower in the background

birds in a tree with a beautiful pink flower in the background

birds in a tree with a beautiful pink flower in the background

birds in a tree with a beautiful pink
=========================
=== Inference Example beach.png ===
Image Path: beach.png
Generated Text: the beach is a popular destination for many people.

the beach is a popular destination for many people.

the beach is a popular destination for many people.

the beach is a popular destination for many people.

the beach
=========================
```

### ToDos

- [ ] Finetune the model similar to Llava paper on downstream tasks
- [ ] Benchmark the results on downstream tasks
